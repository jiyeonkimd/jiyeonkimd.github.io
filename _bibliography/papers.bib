---
---

@string{aps = {American Physical Society,}}

@article{Kim2024KnowledgeED,
  title={Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition},
  author={Jiyeon Kim and Hyunji Lee and Hyowon Cho and Joel Jang and Hyeonbin Hwang and Seungpil Won and Youbin Ahn and Dohaeng Lee and Minjoon Seo},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.01380},
  url={https://api.semanticscholar.org/CorpusID:273025776}
}

@inproceedings{Lee2024HowDV,
  title={How Does Vision-Language Adaptation Impact the Safety of Vision Language Models?},
  author={Seongyun Lee and Geewook Kim and Jiyeon Kim and Hyunji Lee and Hoyeon Chang and Sue Hyun Park and Minjoon Seo},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:273233166}
}

@inproceedings{yoon-etal-2024-listt5,
    title = "{L}ist{T}5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval",
    author = "Yoon, Soyoung  and
      Choi, Eunbi  and
      Kim, Jiyeon  and
      Yun, Hyeongu  and
      Kim, Yireun  and
      Hwang, Seung-won",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.125",
    doi = "10.18653/v1/2024.acl-long.125",
    pages = "2287--2308",
    abstract = "We propose ListT5, a novel reranking approach based on Fusion-in-Decoder (FiD) that handles multiple candidate passages at both train and inference time. We also introduce an efficient inference framework for listwise ranking based on m-ary tournament sort with output caching. We evaluate and compare our model on the BEIR benchmark for zero-shot retrieval task, demonstrating that ListT5 (1) outperforms the state-of-the-art RankT5 baseline with a notable +1.3 gain in the average NDCG@10 score, (2) has an efficiency comparable to pointwise ranking models and surpasses the efficiency of previous listwise ranking models, and (3) overcomes the lost-in-the-middle problem of previous listwise rerankers. Our code, model checkpoints, and the evaluation framework will be fully open-sourced.",
}